{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b51e0e5",
   "metadata": {},
   "source": [
    "# Scenario 1: Retail Inventory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7683b",
   "metadata": {},
   "source": [
    "# Prompt:\n",
    "I need to analyse a retail inventory data to understand stock movement patterns to optimize ordering.  I would like to structure the analysis approach for 1. data validation 2. Calculate inventory turnover rates.\n",
    "3.Identify slow-moving items. \n",
    "4.Predict potential stockouts. \n",
    "5.Create visualizations.\n",
    "\n",
    "# Generated Code:\n",
    "Inventory Data Analysis Approach\n",
    "1. Data Validation\n",
    "Before analysis, ensure the dataset is reliable.\n",
    "\n",
    "Checklist\n",
    "\n",
    "Data Quality Checks\n",
    "\n",
    "Check for missing values (NaN) in key columns (e.g., Item_ID, Quantity, Date, Sales, Stock_Level).\n",
    "\n",
    "Verify data types (dates as datetime, quantities as integers, etc.).\n",
    "\n",
    "Ensure no duplicate rows unless expected (e.g., multiple sales in the same day).\n",
    "\n",
    "Logical Consistency\n",
    "\n",
    "Stock levels should not go negative unless returns/backorders are explicitly tracked.\n",
    "\n",
    "Sales should not exceed available stock (unless overselling is allowed).\n",
    "\n",
    "Date Coverage\n",
    "\n",
    "Ensure the data covers a continuous period (no unexpected gaps).\n",
    "\n",
    "Outlier Detection\n",
    "\n",
    "Identify abnormal spikes in sales or stock counts.\n",
    "\n",
    "Implementation Tip: Use pandas functions like df.info(), df.describe(), df.isnull().sum(), and group-level checks.\n",
    "\n",
    "2. Calculate Inventory Turnover Rates\n",
    "The inventory turnover ratio measures how efficiently stock is being sold.\n",
    "\n",
    "Formula:\n",
    "\n",
    "InventoryÂ Turnover\n",
    "=\n",
    "CostÂ ofÂ GoodsÂ SoldÂ (COGS)\n",
    "AverageÂ Inventory\n",
    "InventoryÂ Turnover= \n",
    "AverageÂ Inventory\n",
    "CostÂ ofÂ GoodsÂ SoldÂ (COGS)\n",
    "â€‹\n",
    " \n",
    "If COGS isnâ€™t available, use Sales Quantity instead.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Compute Average Inventory = (Opening Stock + Closing Stock) / 2\n",
    "\n",
    "Compute Turnover Ratio = Total Units Sold / Average Inventory\n",
    "\n",
    "Do this per product and at category/overall level.\n",
    "\n",
    "Implementation Tip: Group by Item_ID and compute sums and averages.\n",
    "\n",
    "3. Identify Slow-Moving Items\n",
    "These are products that sit in inventory too long.\n",
    "\n",
    "Criteria (choose 1 or more):\n",
    "\n",
    "Low turnover ratio (e.g., bottom 20%).\n",
    "\n",
    "Days Inventory Outstanding (DIO) is high:\n",
    "\n",
    "DIO\n",
    "=\n",
    "ğ´\n",
    "ğ‘£\n",
    "ğ‘’\n",
    "ğ‘Ÿ\n",
    "ğ‘\n",
    "ğ‘”\n",
    "ğ‘’\n",
    "ğ¼\n",
    "ğ‘›\n",
    "ğ‘£\n",
    "ğ‘’\n",
    "ğ‘›\n",
    "ğ‘¡\n",
    "ğ‘œ\n",
    "ğ‘Ÿ\n",
    "ğ‘¦\n",
    "ğ·\n",
    "ğ‘\n",
    "ğ‘–\n",
    "ğ‘™\n",
    "ğ‘¦\n",
    "ğ‘†\n",
    "ğ‘\n",
    "ğ‘™\n",
    "ğ‘’\n",
    "ğ‘ \n",
    "DIO= \n",
    "DailySales\n",
    "AverageInventory\n",
    "â€‹\n",
    " \n",
    "Items with sales below a threshold over last 90 days.\n",
    "\n",
    "Output:\n",
    "\n",
    "List of products flagged as slow-moving.\n",
    "\n",
    "Possibly group by category to see if trend is category-specific.\n",
    "\n",
    "4. Predict Potential Stockouts\n",
    "Here, youâ€™ll forecast when stock will run out if sales trends continue.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Calculate Daily Average Sales (units/day) for each item.\n",
    "\n",
    "DailyÂ SalesÂ Avg\n",
    "=\n",
    "TotalÂ SalesÂ inÂ Period\n",
    "NumberÂ ofÂ Days\n",
    "DailyÂ SalesÂ Avg= \n",
    "NumberÂ ofÂ Days\n",
    "TotalÂ SalesÂ inÂ Period\n",
    "â€‹\n",
    " \n",
    "Estimate Days Until Stockout:\n",
    "\n",
    "DaysÂ UntilÂ Stockout\n",
    "=\n",
    "CurrentÂ StockÂ Level\n",
    "DailyÂ SalesÂ Avg\n",
    "DaysÂ UntilÂ Stockout= \n",
    "DailyÂ SalesÂ Avg\n",
    "CurrentÂ StockÂ Level\n",
    "â€‹\n",
    " \n",
    "Flag items with < X days remaining (e.g., less than 14 days).\n",
    "\n",
    "Optional: Use a time-series forecast (ARIMA, Prophet) for high-demand products to model seasonality.\n",
    "\n",
    "5. Create Visualizations\n",
    "Visuals make patterns clear for stakeholders.\n",
    "\n",
    "Suggested Dashboards\n",
    "\n",
    "Inventory Turnover Trends\n",
    "\n",
    "Bar chart of turnover ratio per product/category.\n",
    "\n",
    "Slow-Moving vs Fast-Moving Items\n",
    "\n",
    "Scatter plot: X-axis = turnover, Y-axis = sales volume.\n",
    "\n",
    "Stockout Risk Heatmap\n",
    "\n",
    "Highlight items by urgency of replenishment.\n",
    "\n",
    "Stock Levels vs. Sales\n",
    "\n",
    "Line chart showing stock over time with sales overlay.\n",
    "\n",
    "Category Contribution\n",
    "\n",
    "Pie or tree map of sales vs. inventory share.\n",
    "\n",
    "Tools:\n",
    "\n",
    "Python: matplotlib, plotly, seaborn.\n",
    "\n",
    "Power BI or Tableau: For interactive dashboards.\n",
    "\n",
    "âœ… Deliverable Workflow\n",
    "\n",
    "Validate & clean the dataset.\n",
    "\n",
    "Compute turnover ratios and DIO.\n",
    "\n",
    "Flag slow movers.\n",
    "\n",
    "Predict stockouts.\n",
    "\n",
    "Build a dashboard for decision-makers.\n",
    "\n",
    "âš¡ Question for you:\n",
    "Do you want me to give you sample Python code (pandas + matplotlib) for these steps, or do you plan to build the dashboard in Power BI/Tableau and just need the analytical logic?\n",
    "\n",
    "# Follow-up Prompts:\n",
    "\n",
    "I have retail inventory data with columns: \n",
    "\n",
    "product_id\n",
    "category\n",
    "stock_level\n",
    "last_restock_date\n",
    "sales_last_30_days\n",
    "supplier_lead_time\n",
    "unit_cost, help me write Python code that can load and validate this data for  analysis . Use Pandas to load in the data . Ensure proper validation and error handling and for the visual i would like to use matplotlib or seaborn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6426e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Final Solution:\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Step 1: Load Data ===\n",
    "# Replace this with your real CSV/Excel file path\n",
    "# df = pd.read_csv(\"your_inventory_file.csv\")\n",
    "\n",
    "# Example sample data (remove this when using real data)\n",
    "data = {\n",
    "    \"product_id\": [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\"],\n",
    "    \"category\": [\"Electronics\", \"Clothing\", \"Clothing\", \"Grocery\", \"Electronics\"],\n",
    "    \"stock_level\": [120, 50, 200, 30, 15],\n",
    "    \"last_restock_date\": [\"2025-07-01\", \"2025-06-15\", \"2025-07-10\", \"2025-07-05\", \"2025-06-25\"],\n",
    "    \"sales_last_30_days\": [5, 40, 20, 25, 30],\n",
    "    \"supplier_lead_time\": [15, 7, 20, 5, 10],\n",
    "    \"unit_cost\": [200, 30, 25, 10, 500]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df[\"last_restock_date\"] = pd.to_datetime(df[\"last_restock_date\"], errors=\"coerce\")\n",
    "\n",
    "# === Step 2: Identify Slow-Moving Items ===\n",
    "df[\"turnover_ratio\"] = df[\"sales_last_30_days\"] / df[\"stock_level\"].replace(0, pd.NA)\n",
    "threshold = df[\"turnover_ratio\"].quantile(0.2)  # bottom 20% cutoff\n",
    "df[\"slow_mover_flag\"] = df[\"turnover_ratio\"] <= threshold\n",
    "\n",
    "# === Step 3: Predict Potential Stockouts ===\n",
    "df[\"daily_sales_avg\"] = df[\"sales_last_30_days\"] / 30\n",
    "df[\"days_until_stockout\"] = df[\"stock_level\"] / df[\"daily_sales_avg\"].replace(0, pd.NA)\n",
    "df[\"stockout_risk\"] = df[\"days_until_stockout\"] <= df[\"supplier_lead_time\"]\n",
    "\n",
    "# === Step 4: Visualizations ===\n",
    "# 1. Turnover Ratio Bar Chart\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=df, x=\"product_id\", y=\"turnover_ratio\", hue=\"slow_mover_flag\")\n",
    "plt.title(\"Inventory Turnover Ratio (Slow Movers Highlighted)\")\n",
    "plt.xlabel(\"Product ID\")\n",
    "plt.ylabel(\"Turnover Ratio\")\n",
    "plt.legend(title=\"Slow Mover\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Scatter Plot: Stock vs Daily Sales\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(data=df, x=\"daily_sales_avg\", y=\"stock_level\", hue=\"stockout_risk\", style=\"category\", s=100)\n",
    "plt.title(\"Stock Levels vs Daily Sales (Stockout Risk Highlighted)\")\n",
    "plt.xlabel(\"Daily Sales Average\")\n",
    "plt.ylabel(\"Stock Level\")\n",
    "plt.legend(title=\"Stockout Risk\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Days Until Stockout Bar Chart\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(data=df, x=\"product_id\", y=\"days_until_stockout\", hue=\"stockout_risk\")\n",
    "plt.axhline(y=df[\"supplier_lead_time\"].mean(), color=\"red\", linestyle=\"--\", label=\"Avg Supplier Lead Time\")\n",
    "plt.title(\"Predicted Days Until Stockout\")\n",
    "plt.xlabel(\"Product ID\")\n",
    "plt.ylabel(\"Days Until Stockout\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# === Step 5: Output Summary ===\n",
    "df_result = df[[\n",
    "    \"product_id\", \"category\", \"stock_level\", \"sales_last_30_days\",\n",
    "    \"turnover_ratio\", \"slow_mover_flag\", \"days_until_stockout\", \"stockout_risk\"\n",
    "]]\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed6bb4a",
   "metadata": {},
   "source": [
    "# Scenario 2: Website Analytics Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672eb2f8",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "\n",
    "def analyze_user_engagement(logs_df):\n",
    "    metrics = {\n",
    "        'bounce_rate': logs_df.groupby('session_id')['page_views'].apply(\n",
    "            lambda x: x == 1).mean(),\n",
    "        'avg_session_time': logs_df.groupby('session_id')['duration'].sum(),\n",
    "        'pages_per_session': logs_df.groupby('session_id')['page_views'].mean()\n",
    "    }\n",
    "    \n",
    "    device_metrics = logs_df.groupby('device_type').agg({\n",
    "        'session_id': 'count',\n",
    "        'duration': 'mean',\n",
    "        'page_views': 'sum'\n",
    "    })\n",
    "    \n",
    "    return metrics, device_metrics\n",
    "   \n",
    "    Help me debug and fix this return rate calculation code . the current code is showing bounce rates over 100%, negative session times and seems incorrect. include and explanation of the fix that was implemented. i have provided the code here\n",
    "\n",
    "# Generated Code:\n",
    "\n",
    "Issues in Current Code\n",
    "Bounce Rate Calculation\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "logs_df.groupby('session_id')['page_views'].apply(lambda x: x == 1).mean()\n",
    "Problem:\n",
    "\n",
    "x is a Series of values per session, so x == 1 produces a Series of booleans, not a single True/False.\n",
    "\n",
    ".mean() is then applied across all booleans rather than per session.\n",
    "\n",
    "Result: It counts every page view, not per-session bounce, which inflates bounce rate above 100%.\n",
    "\n",
    "âœ… Fix: Count sessions with only 1 page view, then divide by total sessions.\n",
    "\n",
    "Average Session Time\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "logs_df.groupby('session_id')['duration'].sum()\n",
    "Problem:\n",
    "\n",
    "This produces a Series of total durations per session, not a single average value.\n",
    "\n",
    "No .mean() applied â†’ returns raw totals.\n",
    "\n",
    "If durations can be negative (from faulty logs), youâ€™ll see negatives.\n",
    "\n",
    "Result: Not actually the \"average session time.\"\n",
    "\n",
    "âœ… Fix: Take the mean of total session durations, and ensure negatives are treated as 0 or cleaned.\n",
    "\n",
    "Pages per Session\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "logs_df.groupby('session_id')['page_views'].mean()\n",
    "Problem:\n",
    "\n",
    ".mean() on a grouped Series gives a Series, not a metric.\n",
    "\n",
    "You likely want the average pages per session across all sessions.\n",
    "\n",
    "Result: You get per-session averages, not a global metric.\n",
    "\n",
    "âœ… Fix: Sum per session first, then take the mean across sessions.\n",
    "\n",
    "Device Metrics\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "device_metrics = logs_df.groupby('device_type').agg({\n",
    "    'session_id': 'count',\n",
    "    'duration': 'mean',\n",
    "    'page_views': 'sum'\n",
    "})\n",
    "Problem:\n",
    "\n",
    "Counting session_id like this counts rows, not unique sessions.\n",
    "\n",
    "Result: Overcounted sessions if a session has multiple rows.\n",
    "\n",
    "âœ… Fix: Use .nunique() for session counts.\n",
    "\n",
    "âœ… Fixed Code\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def analyze_user_engagement(logs_df):\n",
    "    # Ensure duration is non-negative (cleaning bad logs)\n",
    "    logs_df['duration'] = logs_df['duration'].clip(lower=0)\n",
    "\n",
    "    # Aggregate per session\n",
    "    session_stats = logs_df.groupby('session_id').agg({\n",
    "        'page_views': 'sum',\n",
    "        'duration': 'sum'\n",
    "    })\n",
    "\n",
    "    # Bounce rate: sessions with only 1 page view\n",
    "    total_sessions = session_stats.shape[0]\n",
    "    bounce_sessions = (session_stats['page_views'] == 1).sum()\n",
    "    bounce_rate = bounce_sessions / total_sessions if total_sessions > 0 else 0\n",
    "\n",
    "    # Average session time\n",
    "    avg_session_time = session_stats['duration'].mean()\n",
    "\n",
    "    # Pages per session\n",
    "    pages_per_session = session_stats['page_views'].mean()\n",
    "\n",
    "    metrics = {\n",
    "        'bounce_rate': bounce_rate,\n",
    "        'avg_session_time': avg_session_time,\n",
    "        'pages_per_session': pages_per_session\n",
    "    }\n",
    "\n",
    "    # Device-level metrics\n",
    "    device_metrics = logs_df.groupby('device_type').agg(\n",
    "        sessions=('session_id', 'nunique'),\n",
    "        avg_duration=('duration', 'mean'),\n",
    "        total_page_views=('page_views', 'sum')\n",
    "    )\n",
    "\n",
    "    return metrics, device_metrics\n",
    "âœ… Explanation of Fixes\n",
    "Bounce Rate\n",
    "\n",
    "Old: Compared each row â†’ could exceed 100%.\n",
    "\n",
    "New: Compare total page views per session to 1 â†’ proper bounce rate.\n",
    "\n",
    "Average Session Time\n",
    "\n",
    "Old: Returned a Series of sums (no aggregation).\n",
    "\n",
    "New: Compute mean session duration across sessions.\n",
    "\n",
    "Pages per Session\n",
    "\n",
    "Old: Mean per row (inflated if sessions had many rows).\n",
    "\n",
    "New: Sum per session â†’ then average across sessions.\n",
    "\n",
    "Device Metrics\n",
    "\n",
    "Old: Counted rows â†’ overcounted.\n",
    "\n",
    "New: nunique for sessions â†’ accurate counts.\n",
    "\n",
    "Negative Durations\n",
    "\n",
    "Added .clip(lower=0) to clean corrupted values.\n",
    "\n",
    " This version ensures bounce rates are between 0â€“100%, session times are realistic, and device-level stats arenâ€™t inflated.\n",
    "\n",
    "Do you want me to also add a seaborn visualization block (bounce rate bar chart & session duration distribution) inside this function, or do you prefer to keep analytics and visuals separate?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e151c2",
   "metadata": {},
   "source": [
    "# Scenario 3: Customer Segmentation Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca13ce",
   "metadata": {},
   "source": [
    "# Prompt\n",
    " Help me write a SQL query to segment customers based on their purchasing behavior for a new feature rollout on the given conditions below. The SQL database contains the following tables and fields : \n",
    " user_activity\n",
    " user_id\n",
    "last_login_date\n",
    "feature_usage_count\n",
    "account_type\n",
    "transactions\n",
    " transaction_id\n",
    "user_id\n",
    "transaction_date\n",
    "amount\n",
    "platform\n",
    "user_preferences\n",
    " user_id\n",
    "communication_preference\n",
    "interface_theme\n",
    "notification_settings\n",
    "\n",
    "Conditions:\n",
    "\n",
    "Active users (logged in last 30 days)\n",
    "Filter by high-value customers (top 20% by spending)\n",
    "User preference trends for the identified customers\n",
    "\n",
    "# Generated Code\n",
    "\n",
    "Logic Breakdown\n",
    "1. Active Users (last 30 days)\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "ua.last_login_date >= CURRENT_DATE - INTERVAL '30 days'\n",
    "2. High-Value Customers (top 20% by spending)\n",
    "Compute total spend per user from transactions.\n",
    "\n",
    "Use a window function to rank users by total spending.\n",
    "\n",
    "Select the top 20%.\n",
    "\n",
    "3. User Preference Trends\n",
    "Join with user_preferences to see preferences for those customers.\n",
    "\n",
    "âœ… Full SQL Query\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "WITH user_spending AS (\n",
    "    SELECT \n",
    "        t.user_id,\n",
    "        SUM(t.amount) AS total_spent\n",
    "    FROM transactions t\n",
    "    GROUP BY t.user_id\n",
    "),\n",
    "\n",
    "spending_rank AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        total_spent,\n",
    "        PERCENT_RANK() OVER (ORDER BY total_spent DESC) AS spend_percentile\n",
    "    FROM user_spending\n",
    "),\n",
    "\n",
    "active_high_value_users AS (\n",
    "    SELECT \n",
    "        ua.user_id,\n",
    "        ua.account_type,\n",
    "        ua.last_login_date,\n",
    "        ua.feature_usage_count,\n",
    "        sr.total_spent\n",
    "    FROM user_activity ua\n",
    "    JOIN spending_rank sr ON ua.user_id = sr.user_id\n",
    "    WHERE ua.last_login_date >= CURRENT_DATE - INTERVAL '30 days'\n",
    "      AND sr.spend_percentile <= 0.20  -- top 20% spenders\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    ahv.user_id,\n",
    "    ahv.account_type,\n",
    "    ahv.last_login_date,\n",
    "    ahv.feature_usage_count,\n",
    "    ahv.total_spent,\n",
    "    up.communication_preference,\n",
    "    up.interface_theme,\n",
    "    up.notification_settings\n",
    "FROM active_high_value_users ahv\n",
    "JOIN user_preferences up \n",
    "    ON ahv.user_id = up.user_id\n",
    "ORDER BY ahv.total_spent DESC;\n",
    "Explanation\n",
    "user_spending CTE\n",
    "\n",
    "Aggregates each userâ€™s total spending.\n",
    "\n",
    "spending_rank CTE\n",
    "\n",
    "Uses PERCENT_RANK() to rank customers by spend percentile.\n",
    "\n",
    "Ensures we can filter the top 20% (<= 0.20).\n",
    "\n",
    "active_high_value_users CTE\n",
    "\n",
    "Filters only those who logged in the past 30 days.\n",
    "\n",
    "Joins with spending rank to keep high-value customers.\n",
    "\n",
    "Final SELECT\n",
    "\n",
    "Pulls in user preferences for segmentation insights.\n",
    "\n",
    "âœ… Output Example Fields\n",
    "user_id\taccount_type\tlast_login_date\tfeature_usage_count\ttotal_spent\tcommunication_preference\tinterface_theme\tnotification_settings\n",
    "10234\tPremium\t2025-07-15\t45\t12,500.00\tEmail\tDark\tEnabled\n",
    "\n",
    "âš¡ Question:\n",
    "Do you want me to also group by preference trends (e.g., % of high-value users preferring dark mode vs light mode), or just return the detailed list of users meeting the criteria?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
